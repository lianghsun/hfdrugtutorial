{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9596b47-d0a9-466c-9260-398d74a68999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # <-- plz modify this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d61d9f-101c-4002-ba8d-218e4110259e",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013f919c-8d01-481c-bdea-b7ea98fceed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"text\", \n",
    "    data_files={\n",
    "        \"train\": [\"./dataset/MOSES/moses_train.txt\"], \n",
    "        'test': [\"./dataset/MOSES/moses_test.txt\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55da492-d3dd-4ae5-bd4e-dbf9374da752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-6618089194373bbf\n",
      "WARNING:datasets.builder:Found cached dataset text (/home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88296185c92342cbbeaaa4dd0f98b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    \"text\", \n",
    "    data_files={\n",
    "        \"train\": [\"../GPT2/dataset/IRAK4/irak4_train.txt\"], \n",
    "        'test': [\"../GPT2/dataset/IRAK4/irak4_test.txt\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b103f02-f868-4f70-85fc-35d52270f186",
   "metadata": {},
   "source": [
    "# Setup tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c17eb-a8fe-4331-bbb1-fed0a0411f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SmilesPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4daeea5-3a6d-4245-aaf5-27187162f624",
   "metadata": {},
   "source": [
    "## BPE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bee3d063-9e69-48cf-a3f5-2df51d3cf335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "from typing import List, Optional\n",
    "from transformers import PreTrainedTokenizer\n",
    "from SmilesPE.tokenizer import SPE_Tokenizer\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "class Atomwise_Tokenizer(object):\n",
    "    \"\"\"Run atom-level SMILES tokenization\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructs a atom-level Tokenizer.\n",
    "        \"\"\"\n",
    "        self.regex_pattern = r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\" Basic Tokenization of a SMILES.\n",
    "        \"\"\"\n",
    "        tokens = [token for token in self.regex.findall(text)]\n",
    "        return tokens\n",
    "\n",
    "class SMILES_SPE_Tokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Constructs a SMILES tokenizer. Based on SMILES Pair Encoding (https://github.com/XinhaoLi74/SmilesPE).\n",
    "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n",
    "    should refer to the superclass for more information regarding methods.\n",
    "    Args:\n",
    "        vocab_file (:obj:`string`):\n",
    "            File containing the vocabulary.\n",
    "        spe_file (:obj:`string`):\n",
    "            File containing the trained SMILES Pair Encoding vocabulary.\n",
    "        unk_token (:obj:`string`, `optional`, defaults to \"[UNK]\"):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        sep_token (:obj:`string`, `optional`, defaults to \"[SEP]\"):\n",
    "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
    "            for sequence classification or for a text and a question for question answering.\n",
    "            It is also used as the last token of a sequence built with special tokens.\n",
    "        pad_token (:obj:`string`, `optional`, defaults to \"[PAD]\"):\n",
    "            The token used for padding, for example when batching sequences of different lengths.\n",
    "        cls_token (:obj:`string`, `optional`, defaults to \"[CLS]\"):\n",
    "            The classifier token which is used when doing sequence classification (classification of the whole\n",
    "            sequence instead of per-token classification). It is the first token of the sequence when built with\n",
    "            special tokens.\n",
    "        mask_token (:obj:`string`, `optional`, defaults to \"[MASK]\"):\n",
    "            The token used for masking values. This is the token used when training this model with masked language\n",
    "            modeling. This is the token which the model will try to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        spe_file,\n",
    "        unk_token=\"[UNK]\",\n",
    "        bos_token=\"[BOS]\",\n",
    "        eos_token=\"[EOS]\",\n",
    "        # sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        # cls_token=\"[CLS]\",\n",
    "        # mask_token=\"[MASK]\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            pad_token=pad_token,\n",
    "            # cls_token=cls_token,\n",
    "            # mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'.\".format(vocab_file)\n",
    "            )\n",
    "        if not os.path.isfile(spe_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a SPE vocabulary file at path '{}'.\".format(spe_file)\n",
    "            )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.spe_vocab = codecs.open(spe_file)\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.spe_tokenizer = SPE_Tokenizer(self.spe_vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.vocab, **self.added_tokens_encoder)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return self.spe_tokenizer.tokenize(text).split(' ')\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "        Returns:\n",
    "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        bos = [self.bos_token_id]\n",
    "        eos = [self.eos_token_id]\n",
    "        output = bos + token_ids_0 + eos\n",
    "        if token_ids_1 is None:\n",
    "            return output\n",
    "        return output + [bos] + token_ids_1 + [eos]\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True if the token list is already formatted with special tokens for the model\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        \"\"\"\n",
    "        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
    "        Args:\n",
    "            vocab_path (:obj:`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "        Returns:\n",
    "            :obj:`Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1781c8-ccb8-430e-b8f1-96e18ebda087",
   "metadata": {
    "id": "Dk_XnbONcHAn"
   },
   "outputs": [],
   "source": [
    "tokenizer = SMILES_SPE_Tokenizer(\n",
    "    vocab_file='./tokenization/SPE/vocab_spe.txt', \n",
    "    spe_file= './tokenization/SPE/SPE_ChEMBL.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc01335-137d-4576-89eb-3848dce47a8c",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29580334-bff8-4c79-b7dc-1dffad1d303a",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef8c750-0dd1-4999-9fa2-25346105a8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-214c143e743d62fd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-1884a17d7453e406.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-dca93dc2b1bc9f97.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-20faeb84d0139d07.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-ca38e41d33a6d520.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-169c0d2c10650e49.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-dde3eaca3fcc20c6.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-3363778a5f020f98.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-c2c8fa06b8b099af.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-90195391f5755da5.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-cdbdfeeb7fd3ba06.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-d162e3e9f0ad0d8a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-dc0c373796300356.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-614b684e04702db8.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-b1c5d7b4a8b936ca.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-545ea49901d11223.arrow\n"
     ]
    }
   ],
   "source": [
    "BLOCK_SIZE = 30\n",
    "def tokenize_func(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', max_length=BLOCK_SIZE)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_func, batched=True, num_proc=8, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d016159d-debf-466d-b2ac-68010a4c9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(result):\n",
    "    # Concatenate all texts.\n",
    "    # concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    #     # customize this part to your needs.\n",
    "    # total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    # # Split by chunks of max_len.\n",
    "    # result = {\n",
    "    #     k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "    #     for k, t in concatenated_examples.items()\n",
    "    # }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4acf895b-72ce-426a-b3b1-ed2f2a820e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-9455806f2c79ed23.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-c15269b4527f7e6a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-a4dc124c297a9874.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-07421f8df4d44567.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-3cd6be295924e77b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-dec6e1979b697167.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-4d118dbcf7254a9a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-49c973c2525d5311.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-fbd69a43da92bfac.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-2935b18afc419e1e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-5152b648694ad8d1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-c9ecabd0ac215c86.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-9a7004a5539f3504.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-aac3e18cc897f032.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-32b786543558f7fc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/owen/.cache/huggingface/datasets/text/default-6618089194373bbf/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad/cache-eab1dce717d80d2b.arrow\n"
     ]
    }
   ],
   "source": [
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    remove_columns='token_type_ids'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ca5f9-09e5-4369-b3b5-599d864cfb20",
   "metadata": {},
   "source": [
    "# Model & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28a0e370-f089-485a-8523-b6bbfa6fd997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers import GPTNeoForCausalLM\n",
    "\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config=config)\n",
    "\n",
    "# Or from pretrianed\n",
    "model = GPTNeoForCausalLM.from_pretrained('../GPT2/tmp/gptneo_moses_spe_test/checkpoint-35000/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffd90b-a3d2-41f1-a694-0d4a6e7220fe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17cb26-6952-44b5-8b0c-8d081d26d841",
   "metadata": {},
   "source": [
    "## w/ Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df109bb6-41d6-4312-bbb9-f558ab6dd822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datetime import datetime\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy = \"steps\",\n",
    "    # save_total_limit=2,\n",
    "    output_dir=f\"./save/gptneo-moses-irak4-spe-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    save_strategy='epoch',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6e481-4734-43b8-a781-b182e7904617",
   "metadata": {
    "tags": []
   },
   "source": [
    "## w/o Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e004f524-d9e7-4c4c-9aed-7e1796a00843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0792b3ef-11e2-4f98-8245-00a04f74bea0",
   "metadata": {},
   "source": [
    "## Start to train ðŸš€"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bb27f52-882c-47d0-9dcc-9e0a7c499b83",
   "metadata": {},
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d722b7-0a24-47ff-8c72-61aa67cde0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF GPT-2",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
